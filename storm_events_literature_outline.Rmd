---
title: "NOAA Storm Events in Literature"
output:
  pdf_document: default
  word_document: default
  html_document: default
bibliography: scholar.bib
---
```{r warning = FALSE, message = FALSE}
library(noaastormevents)
library(tidyverse)
library(viridis)
library(lubridate)
events_2019 <- create_storm_data(date_range = c("2019-01-01", "2019-12-31"))
```
# Introduction  
[BA: I absolutely agree with your points here. These will make a nice Intro. Regarding
your question about distinguishing this from the Gall paper, I think our key contribution
will be showing how many of these concerns/mechanisms for bias exist specifically in the
NOAA Storm Events database. They more used examples from other studies that illustrated
some of the biases. We can really hone in on how they play out in the Storm Events 
database specifically. I've added an Issue on the GitHub page suggesting one new analysis
we can do for long-term trends. I'll add some more as I go through the draft as I think
of any.

Also, I love the point you have here about understanding the quirks. Disaster databases
aren't perfect reflections of when and where severe weather events happened. I don't think
that they ever will be (at least not in the near future, and we certainly can't go back 
to fix records of past events to be perfect, although there have been some helpful 
re-analysis projects like Chris Landsea's reanalysis of hurricanes that try to make the
data more consisent over time [and this would be a nice example to add to the temporal
bias section! I've added an Issue on this.])

I also strongly agree with the idea that, if we know the quirks, we can build tools to 
help address them. Bias correction is one thing that's been done to try to address long-term
temporal bias in tornado studies. This might be an example for us to use. Also, even having
the tools to *get* the data is helpful. We might want to frame the "tools" idea in two 
parts: (1) tools to access and explore the data (which we've made through the package) and
(2) tools to help adjust or account for the mechanisms that might otherwise lead to bias
(we don't have these in the package yet, but this could be room for future development.)]

### Opening  

Severe weather events currently cause extensive monetary loss, property and crop damage, interruptions in commerce, and human injury and fatality. Since 1980, the US has experienced 273 weather and climate disasters that each left over $1 billion in damages [@impacts]. In total, these events cost the US over $1.790 trillion and caused 14,223 deaths [@impacts].   

Scientists expect the impacts of weather events to worsen over time as a result of climate change. For example, the increasing presence of greenhouse gases in the atmosphere leads to increasing annual and extreme temperatures. Extreme temperatures lead to a wide range of health complications. Extreme heat can lead to heat cramps, heat exhaustion, heatstroke, and hyperthermia, while extreme cold leads to hypothermia and frosbite [@climatechange]. Extreme temperatures also exacerbate health conditions related to cardiovascular disease, respiratory disease, cerebrovascular disease, and diabetes [@climatechange]. The continually increasing extreme temperatures will increase the number of temperature related illnesses and deaths in the US per year. 

Climate change is also projected to increase the number and severity of extreme natural disasters. In specific, scientists expect the number of naturally ocurring wildfires to increase [@climatechange]. Wildfires emit particulate matter and ozone precursors which decrease air quality. Low air quality harms the human respiratory and cardiovascular systems and decreases overall well being and productivity. Other natural disasters threaten physical and mental health resulting from "damage to property, destruction of assets, loss of infrastructure and public services, social and economic impacts, environmental degradation, and other factors" [@climatechange].   



[BA: This is great. As we draft this, we will want to make sure we aren't too negative about
the data---these are huge and helpful databases, that have taken loads of work to collect. 
However, the process of recording this data is evolving, and our technologies are advancing, 
which are all very good things but which leave remnants in the data that researchers who
want to use it need to understand to use it appropriately.

Also, it we want to focus our opening on one direciton, I think the key one you've identified
is "it is very important to understand how hazards impact humans" (health, agriculture, 
commerce, etc.). I think this would be a great focus for the opening. It's nice and broad, 
and we could talk a bit about how anticipated changes in patterns of these hazards is 
expected with climate change, and how this, in combination with the large impacts we
see from some events in the present, make it critical that we understand their impacts 
better, both the scale of those impacts and the pathways from the hazard to the impact.
For the present-day impact, we might want to talk some about the billion dollar disasters
that the US expereiences each year (see https://www.ncdc.noaa.gov/billions/ for more on 
this). For future threats, we could look at the US Climate and Health Assessment
(https://health2016.globalchange.gov/) and some of the IPCC reports (for example, 
https://www.ipcc.ch/report/managing-the-risks-of-extreme-events-and-disasters-to-advance-climate-change-adaptation/changes-in-impacts-of-climate-extremes-human-systems-and-ecosystems/).

]

### Funnel 

The current impacts of disasters combined with the anticipated increases in their severity and occurence make it critical that we understand the patterns and scale of weather hazards. Continued disaster data collection and research will improve scientists' ability to predict these events and avoid such large fallout and loss.  

Several organizations currently collect disaster data in the US including the National Weather Service, NCEI, and FEMA. Using this disaster data is common in interdisciplinary research because natural hazards impact several fields of study: economics, epidemiology, atmospheric science, agriculture, etc.   

The American Meteorological Society has published various papers analyzing fatalities associated with weather events [@terti2017situation, @ashley2005derecho, @ashley2008fatalities, @gensini2010examination].   
Ashley and Black examined the cause, spatial distributions, and fatalies caused by nonconvective high-wind events in the US from 1980 to 2005. This study found that fatalies associated with nonconvective high-wind events often occur in boats or vehicles. These fatalities are most likely to occur on the West Coast or Northeast because of extratropical cyclones. These areas of the US have large forests and bodies of water with high population densities that are opportune to being highly affected by high winds [@ashley2008fatalities]. Keeping these vulnerabilities in mind, the study outlines possible improvements to high wind warning systems in the areas. These improvements aim to protect people most likely to be injured or killed by nonconvective high-wind events.   

Ashley and Mote conducted a similar study examining derecho events in the US from 1986 to 2003 [@ashley2005derecho]. Derechoes are often overlooked when examining impactful weather events. However, this study revelead how they actually produce damage comparable to tornadoes and hurricanes. Derechoes caused 153 fatalities and over 2,600 injuries over the 18 year span of this study. They were also responsible for as much or more monetary loss than some hurricane and tornado events over the time span. Their study helps to draw attention to this type of weather event in hopes to advance derecho risk assesments.   

Both of these studies, in part, used data from the National Oceanic & Atmospheric Administration (NOAA) Storm Events database to conduct research. This database currently contains information on 48 different storms, significant weather phenomena, rare/unusual weather, and other significant meteorological events across the United States. NOAA has been recording weather information since 1950 and storing it csv files for each year. NOAA has changed its recording strategies several times from only recording tornado events starting in 1950 to its current 48 event types. Following these changes, they have reformatted and standardized events types without changing any specific values or details [@stormeventsdetails]. 

The database assigns each weather phenomenon with a location, date, event type, event ID, episode ID, property damage estimate, crop damage estimate, county name, state, event narative, and episode narrative. The database categorizes large storms as episodes which contain several individual events. For example, a hurricane will be given one episode ID, and the rain, wind, floods, etc. associated with that hurricane will be given event IDs that fall under that episode. 

The US National Weather Service (NWS) collects weather data for NOAA from storm trackers, federal agencies, the media, the public, and several other sources. The NWS then uses this data to create the NOAA Storm Events database that is released as a monthly publication. 

This data allows scientists to examine how weather events impact human life and how we can reduce or avoid these impacts. However, storing disaster data does not go without limitations. This is because the process of recording disaster data and the technology we use to do it are constantly evolving. These factors make it incumbant that scientists understand the biases and limitations in weather datasets to use large amounts of data approprately and portray new findings accurately. 


[TK: other studies I could include-  

* Galateia terti, isabelle ruin, sandrine anquetin, and Jonathan J. Gourley: A SITUATION-BASED ANALYSIS OF FLASH FLOOD FATALITIES IN THE UNITED STATES  

* Victor A. Gensini and Walker S. Ashley: An examination of rip current fatalities in the United States ]


### Challenge 

Gall et al. wrote a paper covering six major biases associated with major disaster databases used in the US [@gall2009losses]. In our paper, we will focus on the NOAA Storm Events database and investigate the presence of mechanisms that could lead to these major biases in the data. We used the noaastormevents package in R to dissect these issues and examine evidence of bias in several hazard types. 



[BA: This is a great start on the Challenge! I recommend that we move the last sentence 
("It is important to...") into the end of our funnel. This will nicely set readers up to agree
that this study is important and something they want to read. The first sentence ("Gall et al...")
could also move to the funnel. So, then, our funnel might be: 

(1) To better understand the scale and pathways of human impacts from disasters, we need
thorough data recording disaster events; (2) NOAA Storm Events provides this for certain
types of disasters in the US (define which ones, plus some more describing the history and
content of this database); (3) however, as with any disaster database, there are quirks that
have arisen from the recording and collection process of creating this database; (4) to 
appropriately use this database, it is critical for researchers to understand and, if 
possible, account for these quirks (we can try to find a better word than "quirks" at some 
point, but we can use it as a placeholder now). These points would cover a few paragraphs
and nicely funnel us down to our challenge in the last paragraph of the introduction. 

For the challenge itself, I think your middle two sentences ("In this paper..." and 
"We introduce") work for now, but I think we might want to edit these to strengthen them 
as we edit. Right now, they're a bit too focused on the descriptive, where really we're
moving, with our analysis of the data, more toward an active investigation to identify 
evidence in this dataset of mechanisms/quirks that could lead to several types of bias
previously identified as things to look out for in disaster databases (through the Gall
paper and others). I think that focus on an active investigation will sound stronger.]

 
# What is the Storm Events Database


[BA: This description is great. I think this part could probably go in the "funnel" of the Introduction.]

The National Oceanic & Atmospheric Administration (NOAA) Storm Events database collects information on storms, significant weather phenomena, rare or unusual weather, and other significant meteorological events across the United States. NOAA stores this information in csv files for each year. It has been recording weather information since 1950. However, the database has undergone several changes since then. In 1950, NOAA only recorded tornado events and now it records 48 different event types. Following these changes, the NOAA Storm Events website states that they have reformatted and standardized events types without changing any specific values or details [@stormeventsdetails]. 

The database assigns each weather phenomenon with a location, date, event type, event ID, episode ID, property damage estimate, crop damage estimate, county name, state, event narative, and episode narrative. The database categorizes large storms as episodes which contain several individual events. For example, a hurricane will be given one episode ID, and the rain, wind, floods, etc associated with that hurricane will be given event IDs that fall under that episode. 

The US National Weather Service (NWS) collects weather data from storm trackers, federal agencies, the media, the public, and several other sources. The NWS then uses this data to create the NOAA Storm Events database that is released as a monthly publication. 



[BA: I think that one of the things it's important to understand about this database is how
they record with a hierarchy, so there will be different listings for one storm both 
across several types of hazards and several geographic locations. For example, a big
hurricane is one storm system but would result in listings for lots of counties and
for multiple hazards (storm surge, wind, extreme rain, etc.) within some of those counties.
They've got ID numbers that help in joining together different listings for the same storm. 
It might be helpful for us to add a figure somewhere early in the draft to explain this 
aspect of how listings are included in the database. You could take a look through the 
noaastormevents to see if there's a figure or illustration there that might make a good
starting point for helping to illustrate that.]

[BA: For this description, it would be helpful to have some more references on its 
history. For early history, since this was mainly the tornado database, info about
it might be under a different name for the database. I'll try to find some papers we 
could look into to get some more of these details and references to add.]



# Use of Storm Events Database in research on societal impacts

[BA: Let's add some examples of papers in which this database was used to identify
storm exposures, and then the paper was looking at how they impacted humans.]



# Concerns with Storm Events Database  

[BA: Could we start on a paragraph here that gives an introduction to this section
about potential biases? One thing here would be to talk about the idea of the probability
of an event being recorded given that it happens.]

Though the NOAA Storm Events database is quite extensive, several biases affect its data. These biases fall under five major categories which are common in weather data. These include hazard bias, temporal bias, threshold bias, accounting bias, and geographic bias. These biases result from various factors including structural changes over time, reporting errors, inherent bias, and others. 
[BA: Great! The above paragraph definitely belongs in the "funnel" of our Introduction, I think.]


The Storm Events website gives a disclaimer that "an effort is made to use the best available information but because of time and resource constraints, information from [the] sources may be unverified by the NWS" [@stormdatafaq]. 



* extensive database  
* several changes over time (in both *which events* are reported and also *what details of reported events* are included. For example, I think that the reports for tornadoes include estimates of intensity, and the scale used for this has changed over time, from no scale (I think) to the Fujita scale to the enhanced Fujita scale. I think there's been some work to smooth over these changes (i.e., re-analysis), but I'm sure it could still cause issues.) 
* not all events may be reported, some may be double counted [Could you explain this point some more? Perhaps add an example?]
* lots of sources 
* inherent bias in data collection in this specific field (some of this is because it's over such a long time period---over 50 years for some event types)


[BA: Maybe it would be helpful for us to start a small table here that lists each 
type of bias that we cover and a short description of each? We can start that as 
an itemized list, because that will be easier to format right now, and then convert it
to a table later.]

- **hazard bias:** [Add a short definition / description]
- **temporal bias:** [Add a short definition / description]
- **threshold bias:** [Add a short definition / description]
- **accounting bias:** [Add a short definition / description]
- **geographic bias:** [Add a short definition / description]




## Hazard Bias   

[BA: I think there might be a few different things going on with this one. Let's talk about how we can make sure we bring these different elements together in describing this type of bias. Here are some of the elements that seemed to me to be coming up here: (1) some types of events might be more likely to be recorded than others; (2) some
types of hazards within an event might be more likely to be recorded than others; (3) for some types of events, maybe there's some encouragement to list it as a single event; (4) for the events that are listed, they can differ a lot in severity, and there's not much information in the database to help figure out how severe they are; (5) some of the listings are coming from different sources (e.g., media rather than the NWS)]


[BA: Could we be more precise with this statement? For example, is this likely to always (or almost always) be an underreporting rather than an overreporting? Also, for this type of bias, maybe we want to clarify that, even when storms are entered into the database, all of their associated hazards might not be entered. That's what's going on with this type of bias, right? If I've understood this correctly, maybe move towards something like, "The Storm Events database aims to record listings for all hazards brought by a particular storm event. However, in some cases, when a storm event is recorded certain types of hazards within the storm can tend to go unreported." Have I gotten the idea right here?]. 


### Introduction

Within the Storm Events database, there may be inaccurate recording in the number of certain hazard types. 
The Storm Events database aims to record listings for all hazards brought by a particular storm event. However, in some cases, when a storm event is recorded certain types of hazards within the storm can tend to go unreported. 


[BA: We may want to add something like, "and, in general, the probability that an event is recorded here might differ by type of event." I'm thinking that this type of hazard would especially come into play for a study where you're trying to *compare* the frequency of two or more hazards with each other, or even trying to see how each is related with an outcome. We might want to see if we can find some examples of studies that have used this database while looking at two or more types of hazards, to help us in thinking through how a reporting bias that's different by type of hazard might bias the results. Many of our example studies focus on one type of hazard (e.g., tornadoes, rip tides), but maybe we can find some that consider two or more in the same study?]

[BA: For our definition of hazard bias---is it that the probability of an event being record if it happens varies by event type/hazard, and also that the quality and quantity about information provided for a recorded event might also vary by event type/hazard? In other words, some hazards may be less likely to be reported as an event in this database, and when an event is recorded, the information recorded for it might be more comprehensive, accurate, and precise for some types of hazards compared to others.]

### P (event recorded | event happened)

The probability that an event is recorded might differ by event type. This could result from several mechanisms.  First, the US government has certain programs that focus on specific hazards more than others.  For example, the NWS is obliged to provide monetary loss estimates for any flood event even if the damage assessment is a 'guesstimate' (Gall, Borden, and Cutter 2009).  This leads to a higher probability of reporting for flood events in Storm Data because of how they get information.  [TK: I need to look into the flood insurance program to see if this also has an effect]   

Additionally, certain characteristics of event types influence how easy or difficult it is to obtain information about the event. [Would these points help explain our observations from the data? For example, why hail is frequent but sneakerwaves and sleet are rare?] For example, it is more difficult to obtain information [any information (e.g., they aren't recorded at all) or loss information specifically (e.g., they are reported, but the info on the losses isn't good)?] on event types that do not cause much physical damage or monetary loss [Do we have a reference for this? Otherwise, we might need to soften the language in this sentence]. This is because insurance companies typically keep strict records of monetary damages caused by weather events [for all types of hazards, or just certain types? In practice in our data, it looks like not a lot of events are reported by insurance companies, so we might want to think about how that plays in with this idea.].  If this is lacking, then the NWS struggles to find exact damage amounts [any info on alternative sources they use?].  Drought hazards exemplify this issue; they are notoriously underreported because there is a lack of physical damage and it is hard to quantify spatial and monetary losses (Gall, Borden, and Cutter 2009).  Avalanches also showcase this issue as they typically occur in remote locations with little physical damage or fatalities. [TK: I just made this assumption, so I need to check the literature] On the other hand, hurricanes or tornadoes that pass through large cities will cause high amounts of physical damage and fatalities. This would attract the attention of insurance companies and produce clear records of damages.   

[TK: I still want to consider the idea that event severity may affect the probability (events that happen closer to larger populations of people may have a higher likelihood of being reported than an event that is in a more secluded area) . I’m still debating if that would go here or under threshold bias.
Points you have brought up: 
* if a certain type of event tends to happen in well-populated versus less-populated areas (or even if it's large enough that its coverage includes at least some well-populated areas, like a hurricane often does)
* more severe events are more likely to be included and different types of hazards having different probabilities of being reported
Examples from R
* damages and fatalities from avalanches or droughts vs tornado or hurricane  

I also still need to look into a tornado paper that we think mentions this idea of severity. This is stemming off of the quote from gensini "fatal weather events are more likely to be reported due to enhanced media attention" [@gensini2010examination] ]  



### Associated Hazards 

Even when severe weather events are entered into the database, all of their associated hazards might not be entered or their events may be double counted.  NOAA records one large event as an episode and all of its associated hazards as events under than episode.   The NWS documentation goes into detail on this recording structure. 

> "2.9.1 Episode Narrative. Generate an episode with a narrative; otherwise individual events cannot be entered into the Storm Data software. An episode narrative describes the entire episode in a general fashion, and briefly describes the synoptic meteorology associated with the episode. Information in the episode narrative can be very useful for researchers and other users of Storm Data. This narrative does not need to be long or elaborate, rather make it brief and informative. An example would be “A strong cold front passed through the Washington, D.C. area, triggering several instances of damaging thunderstorm winds and large, baseball-size hail.”
> “To ensure events being logged in a single episode are part of the same synoptic meteorological system, events within the same episode may begin no more than five (5) calendar days apart. This will enable the Storm Data preparer to properly document events that double back into a specific region or events that are very slow moving. Examples include Hurricane and Winter Storm events.” [@nwsinstruction]

[Here are more quotes from that section that I might want to weave in:  

> The episode narrative will appear in the Storm Data publication after all events contained within the episode. The episode narrative does not appear in the examples shown in Appendix A, which is reserved for only event narratives. Additionally, a brief summary of fatalities and injuries should be part of the episode narrative for zone-based events.

> 2.9.2 Event Narrative. Detailed information pertaining specifically to the event and not the
overall episode will appear in an event narrative. The event narrative describes the significance or impact of an event within an episode. An event narrative is required for all Tornado events, all Thunderstorm Wind events, and all Lightning events, whether over land or marine zones.  
This narrative will appear immediately below the header-strip in the publication and should
contain descriptive information about the times, locations, and severity of destruction of
property, trees, crops, power lines, roads, bridges, etc. Additionally, a brief summary of fatalities and injuries should be part of the event narrative for county-based and marine zone-based events. For Thunderstorm Wind events with estimated gusts, use sentences such as “Several trees were toppled by powerful downburst gusts.”]

Categorizing these events and episodes can be unclear.  Events could get double counted outside of the episode. Gall et. all provides an example that  hurricane wind, storm surge, and a tornado that were all caused by a hurricane could be counted as three separate events even though they all stemmed from one hurricane [@gall2009losses]. 

[TK: This idea was also mentioned in the Konisky article ("one episode can correlates to mulitple events") so I want to look into that again) [@konisky2016extreme]  ]

To showcase this idea, for the episodes with the most events in 2019, the following graph shows the number of events reported for the episode. This figure demonstrates how one large weather ‘episode’ can include several other events types. For example, Episode 133801 in this figure indicates several reports of flood, debris flow, strong winds, high wind, and hail. All of these different event types occurred as a result of one large storm labelled episode 133801. This unique episode ID helps related events from getting double counted as separate entities.

       
#### Number of events reported per episode ID:   

```{r echo = FALSE}
z_events_2019 <- events_2019 %>%
  dplyr::select_(~ BEGIN_YEARMONTH, ~ BEGIN_DAY, ~ END_YEARMONTH, ~ END_DAY,
                   ~ EPISODE_ID, ~EVENT_ID, ~ STATE, ~ CZ_TYPE, ~ CZ_NAME,
                    ~ DEATHS_DIRECT, ~ DEATHS_INDIRECT, ~ INJURIES_DIRECT,
                  ~ INJURIES_INDIRECT, ~ DAMAGE_PROPERTY, ~ DAMAGE_CROPS,
                   ~ EVENT_TYPE, ~ STATE_FIPS, ~ CZ_FIPS, ~ SOURCE,
                   ~ EPISODE_NARRATIVE, ~ EVENT_NARRATIVE) %>%
  setNames(tolower(names(.))) %>%
  dplyr::filter_(~ cz_type == "Z") %>%
  match_forecast_county()
```

```{r echo = FALSE}
c_events_2019 <- events_2019 %>%
  dplyr::select_(~ BEGIN_YEARMONTH, ~ BEGIN_DAY, ~ END_YEARMONTH, ~ END_DAY,
                   ~ EPISODE_ID, ~EVENT_ID, ~ STATE, ~ CZ_TYPE, ~ CZ_NAME,
                  ~ DEATHS_DIRECT, ~ DEATHS_INDIRECT, ~ INJURIES_DIRECT,
                  ~ INJURIES_INDIRECT, ~ DAMAGE_PROPERTY, ~ DAMAGE_CROPS,
                   ~ EVENT_TYPE, ~ STATE_FIPS, ~ CZ_FIPS, ~ SOURCE,
                   ~ EPISODE_NARRATIVE, ~ EVENT_NARRATIVE) %>%
  setNames(tolower(names(.))) %>%
  dplyr::filter_(~ cz_type == "C") %>%
  dplyr::mutate_(fips = ~ as.numeric(paste0(state_fips, str_pad(cz_fips, 3, pad = "0"))))

county_events_2019 <- bind_rows(c_events_2019, filter(z_events_2019, !is.na(fips)))
```

```{r echo = FALSE}
county_damage_2019 <- county_events_2019 %>%
  dplyr::select(episode_id , event_id, fips, event_type, contains("damage")) %>% 
  tidyr::gather(key = impact, value = amount, -event_type, -event_id, -episode_id, -fips) %>%
  dplyr::mutate(amount = parse_damage(amount)) 
```

```{r echo = FALSE}
county_events_2019 %>%
  filter(episode_id == "99142" & fips == 2023) %>%
  select(event_type, contains("damage"), cz_name, state, event_narrative, episode_narrative) %>% 
  as.data.frame() %>% 
  pander::pander()
```

```{r echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 4}
top_episodes <- events_2019 %>%
  group_by(EPISODE_ID) %>%
  summarize(n = n()) %>%
  arrange(desc(n)) %>%
  slice(1:10)

a <- events_2019 %>%
  filter(EPISODE_ID %in% top_episodes$EPISODE_ID) %>%
  group_by(EPISODE_ID, EVENT_TYPE) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  spread(key = EPISODE_ID, value = n, fill = 0) %>%
  tibble::remove_rownames() %>%
  as.data.frame() %>%
  tibble::column_to_rownames(var = "EVENT_TYPE") %>%
  dist("canberra") %>%
  hclust() 
```

```{r echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 3.5}
events_2019 %>%
  filter(EPISODE_ID %in% top_episodes$EPISODE_ID) %>%
  group_by(EPISODE_ID, EVENT_TYPE) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  mutate(EPISODE_ID = factor(EPISODE_ID, levels = unique(top_episodes$EPISODE_ID)),
         EVENT_TYPE = factor(EVENT_TYPE, levels = unique(a$labels[a$order]))) %>%
  ggplot(aes(x = EPISODE_ID, y = EVENT_TYPE)) + 
  geom_tile(aes(fill = n)) + 
  theme_classic() + 
  labs(x = "", y = "", fill = "Number of events reported") + 
  theme(axis.text.x  = element_text(angle=45, vjust = 1, hjust = 0),
        legend.position = "top",
        plot.margin = unit(c(0, 1, 0, 0), "cm")) +
  scale_x_discrete(position = "top") +
  scale_fill_viridis(option = "A", direction = -1)
```      


#### Cluster Analysis 

Once we removed event types with less that 50 listings in 2019, we did a cluster analysis of event types, to group events that are more likely to occur together within an episode. The following plot shows the resulting cluster structure of these event types. The figure shows that there are certain event types which are very commonly reported together such as high wind and strong wind or heat and excessive heat. These types of events are likely to be reported together becuase they are likely to occur at the same time. It is important to keep these trends in mind as there may be overlap or uncertainty in how to categorize what happened during an episode. 


```{r echo = FALSE, fig.align = "center", fig.width = 6, fig.height = 10}
b <- events_2019 %>%
  group_by(EVENT_TYPE) %>%
  mutate(n_events = n()) %>%
  ungroup() %>%
  filter(n_events >= 50) %>%
  group_by(EPISODE_ID, EVENT_TYPE) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  spread(key = EPISODE_ID, value = n, fill = 0) %>%
  tibble::remove_rownames() %>%
  as.data.frame() %>%
  tibble::column_to_rownames(var = "EVENT_TYPE") %>%
  dist("canberra") %>%
  hclust() 
plot(as.dendrogram(b, hang = 0.25), horiz = TRUE, axes = FALSE)
```



    - it appears that NOAA does try to account for this: "In the event it is obvious that a continuous or nearly continuous swath of thunderstorm wind or hail damage occurred, a single event should be entered into Storm Data. This single event would be described as occurring from Point A to Point B, during Time C to Time D. The related event narrative could describe the width and length of the damage swath. Scientifically, a swath is more accurate and reduces the chance of a researcher interpreting a single event as a series of events occurring across multiple points." 
 
  
3. difficulty of sorting events
  
    - this could include the ideas of severity and source [BA: For the source part, we could look at how different types of events vary in their probability of being reported by different sources (I think we'd talked about that idea for another section, too). There's some code in the Details vignette we can start from for that.]
    
    - there is also limited information on event severity or distinction between events within the database [@luh2015vulnerability]. The NOAA Storm Data Disclaimer states that "Some information appearing in Storm Data may be provided by or gathered from sources outside the National Weather Service (NWS), such as the media, law enforcement and/or other government agencies, private companies, individuals, etc.  [@stormdatafaq]
    
    - Definitional differences [@gall2009losses] [BA: Could you add some more details or explanation on this point? I'm not sure I get it yet.]
    
        * this seems to be more of problem when comparing events across databases. The Gall Losses article gives the example of hurricane-induced storm surges. In SHELDUS, a storm surge gets categorized under "coastal hazards"" and Storm Events categorizes them under "ocean and lake surf". So if someone was trying to detail a large events like a massive hurricane using multiple databases this could create a problem. [BA: Maybe this also comes into play for the different source? Would the media use the same criteria for identifying a disaster as a National Weather Service office? If not, and if the media tend to report certain types of disasters more often than others, than this could maybe create some bias.]
        

    -  NOAA Storm Events defines the types of events that are allowed in the data set. This can be found under Table 1 of Section 2.1.1 of NWS Directive 10-1605 at http://www.nws.noaa.gov/directives/sym/pd01016005curr.pdf. 


#### Event Type: 

```{r echo = FALSE}
events_2019 %>%
  group_by(EVENT_TYPE) %>%
  summarize(N = n()) %>%
  arrange(desc(N)) %>%
  mutate(N = prettyNum(N, big.mark = ",")) %>%
  knitr::kable(col.names = c("Event type", "Number of events in 2019"))
```



## Temporal Bias   

[BA: We might want to start with this type of bias, and then geographic bias. I think those
two are very easy for people to grasp. Then, when we talk about other types of biases, we might
want to talk about them just in terms of things that might cause differences within a certain
period of time and location, so we can separate them from geographic and temporal issues. 
There are a few examples that could really fall into several categories (for example, the 
flood insurance question, which has a time component to it since policies changed over
time), so we might want to discuss any time-related things in this section and start with it.]

[BA: Potential reasons for temporal bias: (1) We've gotten better at "seeing" extreme weather
as it happens---radar, larger networks of monitors, etc. There may be some good examples from 
measuring tornadoes of this idea, where the improvement in weather monitoring 
has helped increase the number of reported events; (2) Some changes have increased the
chance that we notice / report events when they happen. For example, population growth over 
time in some areas might mean there's a higher chance that an event that happens in that
area gets reported. (3) Some temporal changes are structural, in terms of the database
changing it's "rules" for what it recorded. The expansion from just tornadoes to other 
types of events is an example here. This is one where we can help control for any bias, 
since there's clear documentation on some of those changes. However, I wonder if there's still
some kind of lead in period, where it takes them a little while once they start adding an
event to succeed in recording most or all of them? We could maybe check how the rate
of recording changes with time for tornadoes versus some of the "new" events in the 
database in the late 1990s.]

[BA: I like that we distinguish between the idea of long-term trends in the probability 
of an event being recorded in the database (e.g., from one decade to the next) versus seasonal
time trends (e.g., winter versus summer). We probably want to explicitly write a few sentences
introducing that idea that temporal bias can operate at different time scales.]

### Introduction to Temporal Bias: 
Within the NOAA Storm Events database, there are comparable differences in event and loss recording over time. Temporal bias is helpful in describing and understanding these changes within the database. There are long term patterns of temporal bias that may be caused by changes in measuring and recording events and losses. There are also seasonal patterns within the database that relate to unique attributes and temporality of certain weather events.  

### Long Term Trends: 
Long term temporal bias arises because technology is getting better at detecting storm events, population size is growing in high hazard areas, and the NWS has changed their recording strategies and monetary loss acounting. 


Scientists have improved radar and expanded networks of monitors and so they detect more extreme weather events. The advancement of doppler radar and dual-polarization radar technology has increased our ability to detect tornado events [@tornadodetection]. Forcasters and storm spotters have also discovered patterns that help them recognize tornado formation. These advancements increase the tornadoes detected. Verbout et. al states that "the number of tornadoes reported in the United States has doubled from about 600 per year in the 1950s to around 1200 in the 2000s"[@verbout2006evolution]. Technology increases this reported number rather than meteorological changes. 

[BA: Great! This is a great explanation of this idea that we may be able to "see" more events
as we go along. There may be some papers in the climate change field on this topic, too. It 
turns out that this change in technology makes it hard to determine if there has been 
clear evidence of a trend in frequency of some types of extreme events. This comes up a lot
with hurricane research, because we missed a lot of storms that didn't make landfall early in 
the twentieth century and we don't miss them now. I can see if I can find a couple of papers
along those lines that we could add in here.]

Increasing population size and land development also contribute to increased event reporting over time. Population growth increases the probability that an event in an area gets reported. This is because events are reported to the NWS by trained spotters, the public, law enforcement, broadcast media, social media, local and county officials, etc. When the number of these individuals increases in an area, so does the likelihood that they will see and report an event. 

#### How events are reported

Here is a link to the National Weather Service's information on how to become a storm spotter:
[Storm Spotter](https://www.weather.gov/oun/skywarn-spotter)

Acronyms: 
ASOS= "Automated Surface Observing System (ASOS) units are automated sensor suites that are designed to serve meteorological and aviation observing needs" 

Mesonet= "portmanteau of mesoscale network, a network of (typically) automated weather and environmental monitoring stations designed to observe mesoscale meteorological phenomena"

COOP Obbserver= The National Weather Service (NWS) Cooperative Observer Program (Coop) is truly the Nation's weather and climate observing network of, by and for the people. More than 8,700 volunteers take observations on farms, in urban and suburban areas, National Parks, seashores, and mountaintops. The data are truly representative of where people live, work and play.

AWOS= Automated Weather Observing System (AWOS) is a fully configurable airport weather system that provides continuous, real time information and reports on airport weather conditions. AWOS stations are mostly operated, maintained and controlled by aviation service providers.

CoCoRaHS= The Community Collaborative Rain, Hail and Snow Network, or CoCoRaHS, is a network of volunteer weather observers in the United States, Canada, and the Bahamas that take daily readings of precipitation and report them to a central data store over the internet.

SNOTEL= snow telemetry 

RAWS= The Remote Automatic Weather Stations system is a network of automated weather stations run by the U.S. Forest Service and Bureau of Land Management and monitored by the National Interagency Fire Center, mainly to observe potential wildfire conditions.

CMAN Station= The Coastal-Marine Automated Network (C-MAN) is a meteorological observation network along the coastal United States.

AWSS= automated weather sensor system

WLON= a radio station 

SHAVE project= Severe Hazards Analysis & Verification Experiment 

[TK: I did not find much information about these sources in the NWS NOAA documentation. However, from my research it appears that several of these sources seem to be communities of people that are reporting events. This leads to more questions about how objective the reporting is from these groups. There may be differences in how certain people classify certain events. ]


```{r echo = FALSE}
events_2019 %>%
  group_by(SOURCE) %>%
  summarize(N = n()) %>%
  arrange(desc(N)) %>%
  mutate(N = prettyNum(N, big.mark = ",")) %>%
  knitr::kable(col.names = c("Source of event report", "Number of events reported in 2019"))
```



#### More Points for Long Term Trends  

[TK:  

*include information from the US census  

* Here is link to the population data: [US census data](https://www.census.gov/data/tables/time-series/demo/popest/2010s-total-cities-and-towns.html) 

* Here is a link to the locations of National Weather Service offices across the US: [NWS locations](https://www.weather.gov/srh/nwsoffices) ]

[BA: We may want to remind the readers here of the role that storm trackers / reporters play in reporting some of these events. I think that for some types of events, people can volunteer as "spotters" and get some training and then be able to report events they see. I think in one vignette, there's some code that helps in seeing how many of the event reports come from those types of inputs. The population size question could be really important there. Also, I wonder if there's any link between how close National Weather Service offices are and the population density? If so, maybe that is another link with population size / density?]  

#### Breakdown of percentage of events reported by a given source: 


The following graph shows, for each type of event in 2019, the percent reported by each source. Both axes of the plot are ordered by overall frequency (i.e., overall number of each type of event and overall number of reports from each source).
 
For some types of events, reporting is dominated by a specific source. For example, most high surf reports come from trained spotters, while most drought reports come from drought monitors and most tornado reports come from the NWS Storm Survey. Another example is that rip currents are mostly reported by broadcast media and newspapers. This may be because rip currents can pose a serious danger to humans and cause death in certain instances. These instances are more likely to make the news than other types of general events. 

For other types of events, reporting sources are more diversified.

Additionally, certain sources are more influential in reporting certain events. For example, the public often reports hail and marine hail events. This may be because the public is usually highly affected by this type of weather event. 


```{r fig.height = 7, fig.width = 10, echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center"}
tot_events <- events_2019 %>%
  group_by(EVENT_TYPE) %>%
  summarize(tot_events = n()) %>%
  arrange(desc(tot_events))
tot_source <- events_2019 %>%
  group_by(SOURCE) %>%
  summarize(tot_source = n()) %>%
  arrange(tot_source)

events_2019 %>%
  group_by(EVENT_TYPE, SOURCE) %>%
  summarize(N = n()) %>%
  ungroup() %>%
  right_join(tot_events, by = "EVENT_TYPE") %>%
  mutate(perc_events = 100 * N / tot_events) %>%
  mutate(EVENT_TYPE = factor(EVENT_TYPE, levels = unique(EVENT_TYPE)),
         SOURCE = factor(SOURCE, levels = unique(tot_source$SOURCE))) %>%
  ggplot(aes(y = SOURCE, x = EVENT_TYPE)) + 
  geom_tile(aes(fill = perc_events)) + 
  theme_classic() + 
  labs(x = "", y = "", fill = "% events reported by a given source") + 
  theme(axis.text.x  = element_text(angle=45, vjust = 1, hjust = 0),
        legend.position = "top",
        plot.margin = unit(c(0, 1, 0, 0), "cm")) +
  scale_x_discrete(position = "top") +
  scale_fill_viridis(option = "C", direction = -1)
```

Some temporal changes are structural, in terms of the database changing what types of events are recorded. Since 1950, the database has undergone two major changes in event recording. The first change occured in 1954 when the NOAA database starting recording thunderstorm wind and hail in addition to the tornado events it had already been recording [@stormeventsdetails].Fourty-two years later, the database started recording all 48 event types that it continues to record today [@stormeventsdetails]. 

These major changes were also influenced by smaller changes in funding and policy. One of these changes occured between 1976 and 1979 when the NWS received reduced funding from the US government that decreased flood damage data collection [@downton2005reanalysis]. This decreased flood data collection may have produced an artificially low number of recorded floods during that time. To counteract this, in 1983, Congress began to require The US Army Corps of Engineers to provide yearly flood damage reports in contract with the NWS [@downton2005reanalysis]. This could have the opposite effect in that the data may provide a  more consistent report of flood damage that would not have otherwise been reported. These changes pose inconsistensies in the number of certain storm events over large periods of time.



[BA: We might want to add some examples, here or in one of the more general sections, about how these issues are common across a lot of disaster databases. I've added a few papers that talk about common problems for disaster database, rather than just NOAA Storm Events, and those might help in rounding out our discussion.]



### Seasonal Trends:

Although certain weather events truly exhibit seasonal differences in their meteorology, other factors related to temporal bias may artificially inflate or deflate the number of certain events recorded throughout the year. One of these factors is how the timing of a weather event's hazardous outcomes could attract more attention. The media and the public are more likely to report a weather event when it directly physically affects them. One example of this occurs with rip currents. NOAA exhibits higher rates of rip currents in the summer versus the winter. Gensini and Ashley also found that "summer season weekends are shown to have the most [rip current] fatalities than any other time of the year" [@gensini2010examination]. However, the probablity of a rip current occuring during summer or on the weekend may not be inherently higher. It may be that more people attend the beach and swim in the ocean during the summer and on the weekends and are thus more likely to experience and/or report a rip current. Humans experience the hazardous outcome more in the summer and thus will be more likely to report it during that time. 

* Seasonal Differences in number of events reported 

    - Example of rip currents in summer vs winter  
    
* Direct quote: The strength of rip currents can be seasonal. During hurricane season (from June to November) there is a greater chance for rip currents to develop.[@natgeo]
* Rip currents are related to several envrionmental factors including waves (surf heights, period, direction), beach (slope, orientation, material), water levels (tidal cycle, tide ranges), winds (affect wave breaking) and wind-driven currents alongshore, others like local coastal configuration and beach and promontories by natural or human made. [@nwsripcurrent]
* "The most likely scenario for rip hazards is not high surf but high exposure of beachgoers in the warm water of the summer-fall period. When low-energy, longer-period waves (significant wave heights of 0.5 -1.5 meters in 10-15 second sequences) lead to the highest number of rip incidents. During spring/neap tides or very low daily tidal cycles, a mass rescue event can occur, with hundreds of rescues in several locations on a beach, or at several beaches under the same conditions." [@nwsripcurrent]
* "societal factors (e.g., weekends and holidays) that could change the risk of a rip current fatality. For example, low and high pressure systems off the east coast of the United States can produce onshore flow to many surf zones. Both systems may invoke a high rip current formation risk on the LURCS, but swimmers will be more inclined to enter the water on days when a high pressure system is offshore. The clear skies generally associated with large-scale subsidence of a high pressure system would provide beachgoers with favorable weather for beach activities as opposed to a day amid a low pressure system with stratus clouds and precipitation occurring" [@gensini2010examination] 
       
        
* create table of number reported by lifeguards or media vs month of the year  
        
    - other events that are more or less reported? [BA: We might want to talk about the idea of some events that truly are seasonal in their occurrence, like avalanches and hurricanes, versus some that happen year-round but have varying chance of being reported. Also, you can look through the package vignette and see if you see any other examples of things that might not be reported evenly throughout the year.]   
    
    - coastal flood? high surf? waterspout?
    
[BA: I'm not sure if you saw it yet, but I added in another paper on rip currents that to the "literature" folder that might be another helpful reference for this section.]

#### How start dates for events are distributed over the year 

[I don't know if we've explained yet that events are listed with a start and end date. We
might want to explain that here.]
Here are how the start dates for listings for each event type are distributed over the year
(event types are ordered by decreasing total count during the year; note that the y-axes vary depending on the range of events by date for each event type): 

Many event types are clearly seasonal (e.g., winter weather, winter storms, heavy snow, cold, extreme cold, blizzards, ice storms, lake-effect snow, and avalanches are all much more common during winter months, while heat and extreme heat are typically limimted to summer months). However, for some events, reporting seasonal patterns might be based not just on the true pattern of events but also on the timing of important exposures and impacts of the events. For example, rip currents have many more listings during the spring and summer, which may be related to events being more likely to be listed when more people are swimming. Frost event listings are particularly high at the start and end of the frost season, rather than in the middle of winter, which may be related to the impacts of frost on crops being higher in spring and fall than during the winter. If working with this data, it important to keep in mind that the data are based on reporting, and there may be related influences on the probability of an event being reported and included in the data that differ from using data from something like a weather station.  

[BA: Could you check on Google Scholar and see if we could find out anything from the literature
to support our idea here that you're going to get frost through the winter, but that the 
most economically damaging frost comes near the edges of the frost season.]

```{r fig.width = 8, fig.height = 8, echo = FALSE, fig.align = "center"}
events_2019 %>%
  select(BEGIN_DATE_TIME, EVENT_TYPE) %>%
  mutate(date = str_extract(BEGIN_DATE_TIME, ".+\\ "),
         date = dmy(date)) %>%
  group_by(date, EVENT_TYPE) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  group_by(EVENT_TYPE) %>%
  mutate(tot_n = sum(n)) %>%
  ungroup() %>%
  arrange(desc(tot_n)) %>%
  mutate(EVENT_TYPE = factor(EVENT_TYPE, levels = unique(EVENT_TYPE))) %>%
  filter(EVENT_TYPE %in% c("Heat", "Blizzard","Rip Current", "Frost/Freeze")) %>%
  ggplot(aes(x = date, y = n)) + 
  geom_point(alpha = 0.3, size = 2) + 
  facet_wrap(~ EVENT_TYPE, scales = "free_y", ncol = 1) + 
  scale_x_date(date_labels = "%m/%d", date_breaks = "4 months") + 
  theme_classic()
```

### Current techniques for improvement (re-analysis)  

* Christopher Landsea et al. has reanalyzed the Atlantic Hurricane Database for 1911-1930. These studies examine records of hurricanes, determine their accuracy, and readjust if necessary. This readjustment involves removing recorded hurricanes, adding hurricanes, and/or altering the recorded track or intensity.
They do this by 




The goal of this work is to correct systemic biases and random errors in the data. 


## Threshold Bias  
[TK: To consider: Earlier in hazard bias, I discuss event severity by type and how a more severe event may be more likely to be reported. I might want to move those examples here to focus on the role severity plays. Or I could get rid of this section and weave these ideas into other areas.]

The varying severity of an event creates threshold bias in the storm events database. Events of larger maginutude and/or damage to human health are better documented, while events of smaller magnitude are less reported becasue less people are affected [@gall2009losses]. Small events may even be excluded due to threshold criteria in the database [@gall2009losses]. For example, if a flood has received a presidential disaster declaration, FEMA storm survey teams go to the scene and provide extra damage assessments [@downton2005reanalysis]. 

* Events of smaller magnitude are less reported becasue less people are affected [@gall2009losses]   

    - may even be excluded due to threshold criteria [@gall2009losses]  
    
    - verify this under NOAA disclaimer 
    

## Accounting Bias

### Introduction/How damages are reported

There are discrepencies in how monetary losses and damage information are collected in the NWS storm events database that lead to accounting bias.  


This is a quote from the NWS documentation on how estimates are recorded. Not sure if we should include it here or abbove when we describe the database:

"Estimates should be in the form of US Dollar values and rounded to three significant digits,
followed by the magnitude of the value (i.e., 1.55 Billion $USD for $1,550,000,000). Values
used to signify magnitude include: Thousand $USD, Million $USD, and Billion $USD. If
additional precision is available, it may be provided in the narrative part of the entry. When
damage is due to more than one element of the storm, indicate, when possible, the amount of
damage caused by each element. If the dollar amount of damage is unknown, or not available,
check the “no information available” box."

[BA: Overall for this section, I know that there seemed to be some odd things occasionally cropping up in the data from the `noaastormevents` package when you look at damages. You might want to look through the "Details" vignette for anything that specifically talks about damages. I think there were occasionally cases, like Hurricane Floyd in 1999 in North Carolina, where it looked like a lot of the damage was maybe reported for the capital county (the county with Raleigh in it) rather than coastal counties (coastal counties were reported, too, but the numbers were lower, I think). That raised some questions for me about whether sometimes general damages were reported for the state, and just recorded in the capital location, versus in the county where the damage occurred. Also, it may be interesting to see, both from the data and the documentation, (1) whether they split up the total costs across different events in an episode (for example, if there are reports of multiple events during a hurricane for a county, where do they record the damage?) and (2) how different costs are from county to county for the same episode (and particularly for counties that are close to each other but across state lines---do the damages tend to be more similar within a state than between two states?).]


### Inaccuracies in reporting 

The National Weather Service Documentation states that "property damage estimates should be entered as actual dollar amounts, if a reasonably accurate estimate from an insurance company or other qualified individual is available" [@nwsinstruction]. It also states that "if this estimate is not available, then the preparer has two choices: either check the“no information available” box, or make an estimate" [@nwsinstruction].

The only exception to this rule is that the NWS is required to provide monetary damage amounts for flood events by the U.S. Army Corps of Engineers [@nwsinstruction]. Otherwise, the Storm Data preparer is only encouraged to report monetary damage estimates. These estimates can be obtained from emergency managers, U.S. Geological Survey, U.S. Army Corps of Engineers, utility companies, and newspaper articles. 

The estimates of damage to insured property are typically obtained from local field office reports that often lack quality and accuracy control [@downton2005reanalysis]. 

[BA: It would be interesting to see if damages are more likely to be reported based on 
(1) who reports the event and (2) what type of event it is. We could write some code
to check this in our data.]

### Difficulty of quanitfying damages 

The estimates are comprised of direct and indirect monetary losses. Direct monetary losses from damage to infrastructure, buildings, crops, etc. are easier to quanitfy than indirect losses like lost revenue, business closures, societal losses, environmental damage [@gall2009losses].

"Insurance company records include only insured property, and records of the Federal Emergency Management Agency (FEMA) include only property that qualifies for federal assistance in presi- dentially declared disasters. Few state and local governments maintain damage records beyond those required by FEMA; only in newspaper archives from cities and towns across the nation might one find more complete historical reporting of local flood damage." [@downton2005reanalysis]

Monetary losses can also be categorized into community, state, regional, and global levels which can further complicate the precision of the recording [@gall2009losses]. 



### Structural changes in how NOAA reports damages 

Until 1994, damage estimates were recorded on a logarithmic scale [@downton2005reanalysis]. NOAA now records damage estimates in thousands of dollars. [BA: Could you clarify this a bit? I'm not sure what is meant here by the use of a logarithmic scale. Is this in terms of how they rounded the estimates?]

* pull up loss data for events pre and post 1995?  
         
"From 1976 through 1979, NWS reports indicate that reduction of funding led to cutbacks in the compilation of flood damage data. Data collection continued as in prior years, but there appears to have been less checking and updating of initial damage infor- mation. Further, publication of annual summaries ceased. In 1980, compilation of flood damage estimates was discontinued entirely." [@downton2005reanalysis]

"NWS policies on what losses to include have changed some- what over the years. Damage estimates published through 1975 focused primarily on damage to property and crops, but included some indirect losses. Present policy is to focus exclusively on physical damage to property and crops. Until 1992, separate estimates were given for property and agricultural damage, but in 1993 that distinction was eliminated." [@downton2005reanalysis]

* provide example/tabble of how NOAA records financial losses 

### Change in value of the US dollar over time

The value of a dollar has changed over time. This can be addressed by normalizing any dollar measurements to a certain year to prevent bias. [It might be helpful to add some examples of studies that do this. I think there are some from Roger Pielke that look at hurricane damages over time in the US that normalize the damages. We could (briefly) include an explanation of how they do that.]



## Geographic Bias  

### Introduction 

Another area of concern in the NOAA storm events database arises with geographic bias which explains the idea that there may be inconsistencies in the recording of events due to geographic location.  First, there may be differences in the supply of information from different regions based on population differences or locations of weather stations. Secondly, structural changes in the database create differences in how location data is obtained and recorded. 

### Location / Density of people

The reporting of events can be affected by the geographic location of the event itself [@luh2015vulnerability]. In an urban area, more people may be present to witness/experience a storm event than in a rural area. 

* probably a tornado example we can include here [Somewhere we think there's a paper on that]

The reporting of events can be affected by geographic location based on whether or not people are present to record the event. The supply of information is greater in areas closer to the weather event [@konisky2016extreme]. [BA: You could go more into the rural / urban question here. Would the luh2015vulnerability reference be a good example to include for this idea? The tornado example could also go here probably.] 

* this idea is stated in the context of a study about correlations between weather events and public attitudes about climate change. This idea might be helpful in describing geographic bias as more concern is shown towards extreme weather in areas that are more strongly affected? 

[TK: More things get reported where there are more people? Any papers on changes in numbers of media reports of disasters?]

### Structural  

Geographic bias also results from structural changes at the county or state level. In 1995, the NWS changed its reporting strategy from loss estimates by climate region to loss estimates in specific counties where the event occurred [@gall2009losses]. This type of change could lead to excluding loss data or double counting events [@gall2009losses]. The NOAA website even provides a disclaimer that "the source data ingested into the database are widely varied and leads to many questions about the precision and accuracy of the location data" [@stormdatafaq]. 


* script for checking event location pre and post 1995?

[BA: If this is a change that would happen over time, do we want to consider moving it to temporal? Or might it happen at different times in different places? For this, maybe we want to include a map of climate regions? I think a lot of readers might not know how large or small those are off the top of their heads. Also, I really like the idea of comparing event reports before and after this change. Can we see that, before the change, they followed the boundaries of the climate region?]

### Aggregation of reporting 


When a location is entered for an event in the NOAA Storm Events database,  inconsistencies may arise between where the event actually occured and the location that is recorded in the database. Currently, the smallest unit of aggregation to use all parts of the database are called Weather Forecasting Offices. Each office covers a geographic area of the US and there are currently about 122 nationwide [@konisky2016extreme].  These geographic areas overlap with county outlines but do not always directly coincide. It is unclear exactly how NOAA decides when to assign an event to a county or a Weather Forecasting Office. In regards to this decision, the NWS documentation states that "a hydrometeorological event will be referenced, minimally, to the nearest hundredth of a mile, to the geographical center (not from the village/city boundaries or limits) of a particular village/city, airport, inland lake, or location providing that the reference point is documented in the Storm Data software location database" [@nwsinstruction]. 

[BA: In this section, we may also want to talk about how some events might be very localized (for example, a flash flood, or even storm surge that's only near the coast of the county), but the county-level reporting of the event means that all the county gets classified as experiencing or not experiencing the event. I think that sometimes the "Narratives" of the events give details that you could use to find out exactly what parts of the county were affected, and some events like tornadoes give the starting and ending latitude and longitude, so you can figure out the path and where that was in the county. However, overall, it's much easier from the database to get an idea of an event occurrance for the county as a whole rather than for locations within the county.] [We may want to talk a bit about the idea of ecological bias here.] [Maybe want to talk about spatial alignment.]



* include zone and fips script here?   


#### Forecast Zone versus County 

The NOAA Storm Events database also uses forecast zones as another unit of aggregation. Forecast zones are created as subsets of counties for more specific location data. Specific types of events are typically either always reported for a county (`CZTYPE` of "C") or always reported for a forecast zone (`CZTYPE` of "Z") (see table below)**. Events typically reported by county include floods, tornado-like events, and a few other events often related to thunderstorms. Events typically reported by forecast zone include severe winter weather, extreme heat, events related to the water or coast, and a few others ("High Wind", "Dense Fog", "Strong Wind", "Wildfire", "Dust Storm", "Dense Smoke").

This demonstrates the difficulty of narrowing down where exactly certain events occured. It is important to keep in mind that some events might be very localized (for example, a flash flood, or even storm surge that's only near the coast of the county), but the county-level reporting of the event means that the whole county gets classified as experiencing or not experiencing the event.

```{r echo = FALSE}
events_2019 %>%
  group_by(CZ_TYPE, EVENT_TYPE) %>%
  summarize(n_events = n()) %>%
  ungroup() %>%
  spread(key = CZ_TYPE, value = n_events, fill = 0) %>%
  mutate(total = C + Z,
         perc_county = 100 * C / total) %>%
  arrange(desc(perc_county), desc(total)) %>%
  mutate(C = prettyNum(C, big.mark = ","),
         Z = prettyNum(Z, big.mark = ","),
         total = prettyNum(total, big.mark = ","),
         perc_county = paste0(round(perc_county), "%")) %>%
  knitr::kable(col.names = c("Event type", "County", "Forecast Zone", "Total", "% county"),
               align = "lcccc")
```




## Systemic Bias    
[TK: I haven't been able to find a lot of support or evidence for this type of bias. Maybe this could be a good place to describe how these problems span many databases. ]


## Limitations of this paper 

* Some of the examples may not be examples of bias but actual meteorological phenomena that we don't fully understand. 
* I hope that I have researched my examples enough that I am making accurate claims in the eyes of an atmospheric scientist.  
* There may be more forms of bias present in this data that we do not claim 
* May be even other factors at play that make the data appear as it is 

    
# References     











